# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a na√Øve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the na√Øve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given na√Øve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where na√Øve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to na√Øve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT‚Äôs outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.

### üìù Study of Prompt Templating Techniques for Automated Maintenance Report Generation

**1. Define the Scenario and Use Case: Overview**

Domain: Artificial Intelligence, Natural Language Processing (NLP)
Focus: Automated generation of structured maintenance reports using instructional prompt templates.
Target Audience: Industrial maintenance teams, AI researchers, software engineers, data analysts, and enterprise solution architects.

**Scenario & Use Case**

In industries like manufacturing, aviation, energy, and IT infrastructure, regular maintenance reports are critical. Manual report creation is time-consuming, inconsistent, and prone to human errors.

The goal of this project is to design an AI-powered reporting pipeline where different prompt templating patterns (zero-shot, few-shot, chain-of-thought, structured output) are tested to automatically generate clear, consistent, and actionable maintenance reports.

**Primary Use Case:**

A factory maintenance team uploads logs (machine data, error codes, and operator notes). The system applies templated AI prompts to generate a standardized report with diagnosis, recommendations, and future maintenance schedules.

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/9ae08829-3d56-4014-88bf-6cea3b97de89" />


---

### 2. Main Objectives

. ‚úÖ Standardize maintenance report formats across different industries.

. üîÑ Automate repetitive documentation tasks with AI.

. üîç Compare prompt patterns for accuracy, clarity, and efficiency.

. ‚öôÔ∏è Reduce reporting time by >50%.

. üìä Improve decision-making by generating consistent structured outputs.

. üí° Provide a knowledge base for future predictive maintenance systems.

---

### 3. Identify Prompt Patterns for Each Design Aspect**

**Idea Generation Prompts**

. ‚ÄúGenerate a detailed maintenance report from raw log data (temperature spikes, vibration readings, downtime events).‚Äù

. ‚ÄúSuggest a standardized structure for a daily machine maintenance report.‚Äù

. ‚ÄúSummarize operator notes into concise bullet points with recommendations.‚Äù

. ‚ÄúClassify maintenance severity (low, medium, high) based on input data.‚Äù

**Generated Ideas (High-Level Solutions)**

. Zero-Shot Prompting ‚Üí Quick reports without prior examples.

. Few-Shot Prompting ‚Üí Use past sample reports for consistency.

. Chain-of-Thought ‚Üí Encourage reasoning steps for root-cause analysis.

. Structured Output Prompts ‚Üí Generate reports directly in JSON/PDF-ready format.

. Hybrid Prompting ‚Üí Combine templates for clarity + reasoning.

---

### 4. Persona and Context Prompts

**Prompt:** ‚ÄúWhat should the report communicate to operators, managers, and clients for clarity and trust?‚Äù

**For Operators (On-site Technicians):**

. ‚ÄúMachine A showed abnormal vibration (15% above threshold) at 10:42 AM. Recommended immediate inspection of bearings.‚Äù

. Highlight safety risks and urgent tasks.

**For Managers:**

. Summary view: ‚ÄúTotal 5 incidents today. 2 critical, 3 minor. Recommended downtime: 4 hours.‚Äù

. Cost impact and resource allocation info.

. **For Clients (External Reports):**

**Transparent overview:** ‚ÄúMaintenance performed on Line 3. Expected uptime restored to 98% reliability.‚Äù

. Assurance messages with predictive scheduling insights.

---

### 5. Technical Architecture

Steps:

1. Input Layer ‚Üí Raw logs, IoT data, operator notes.

2. Preprocessing ‚Üí Data cleaning & formatting.

3.Prompt Templating Engine ‚Üí Apply chosen instruction pattern.

4. AI Model ‚Üí Generate structured report.

5. Post-Processing ‚Üí Convert into PDF/Excel dashboards.

6. Delivery ‚Üí Reports sent to operators, managers, and clients.

---

### 6. Predictive & Safety Features

. Use structured prompts to ensure safety-critical information is always included.

. Chain-of-Thought ensures diagnosis + root-cause explanation.

. Automated scheduling of next maintenance cycle.

. Consistency checks to minimize false/incomplete reports.

---

### 7. ‚ö†Ô∏è Challenges & Limitations

. Data privacy & security when using cloud models.

. Hallucinations in zero-shot prompts.

. Difficulty in enforcing strict JSON structure.

. Model dependency (different LLMs behave differently).

. High cost for large-scale deployments.

---

### 8. üìà Expected Outcomes

. Consistent report format across teams.

. Time reduction in report preparation (from 2 hrs ‚Üí 10 min).

. Structured outputs ready for BI tools (Excel, PowerBI, Tableau).

. Improved predictive maintenance accuracy.

. Reduction of reporting errors by >40%.

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/bfb4e937-4b3a-41e3-90ef-421c24d5230f" />


---

### ‚úÖ Conclusion

By leveraging instructional prompt patterns, industries can transform the tedious process of maintenance report writing into a fast, accurate, and automated pipeline.
This study highlights the strengths and trade-offs of zero-shot, few-shot, chain-of-thought, and structured output templates in real-world maintenance reporting.

# RESULT: The prompt for the above said problem executed successfully
